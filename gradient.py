# -*- coding: utf-8 -*-
"""gradient

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EIio_0kBrkGD1IOJdHwsbZ_HCxN215LA
"""

#Import libraries
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt

# Define the loss function where X is given the same dimensions as A
def loss_function(X_flat, A_matrix_shape, A_matrix):
    X_reshaped = X_flat.reshape(A_matrix_shape)
    return np.sum((X_reshaped - A_matrix) ** 2) / 2

#Define the gradient function and flatten the result
def gradient(X_flat, A_matrix_shape, A_matrix):
    X_reshaped = X_flat.reshape(A_matrix_shape)
    return (X_reshaped - A_matrix).flatten()


def gradient_descent(X_initial, A, threshold, max_iterations):
    # Start with a copy of X
    X = np.copy(X_initial)

    # Initalize variables and lists
    prev_loss = 0
    loss_values = []

    #Iterate up to max iterations given
    for n in range(max_iterations):

        # Calcualte loss at each iteration and append to a list
        current_loss = loss_function(X.flatten(), A.shape, A)
        loss_values.append(int(current_loss))

        # Break the loop if difference between current and previous loss is
        # below the given threshold
        if abs(prev_loss - current_loss) < threshold:
            print(f"Gradient descent converged after {n} iterations.")
            break

        # Update the previous loss
        prev_loss = current_loss

        # Update X for the next iteration of the outer loop
        result = sp.optimize.minimize(
            fun = loss_function,
            x0 = X.flatten(),
            args = (A.shape, A),
            method = 'BFGS',
            jac = gradient,
            options = {'maxiter': 1, 'disp': False}
        )
        X = result.x.reshape(A.shape)

    else:
        # Display if function never converges in the max iteration count given
        print(f'Gradient descent reached max iterations ({max_iterations}).')

    # Plot the loss values per iteration
    plt.plot(loss_values)
    plt.xlabel("Iteration")
    plt.ylabel("Loss")
    plt.title("Gradient Descent of Loss Function")
    plt.savefig('gradient_descent.png')
    plt.show()

    # Return the list of loss values
    return loss_values

# Initalize X and A using random values
X = np.zeros((100,50))
for i in range(5):
    for j in range(5):
        X[i,j] = np.random.randint(0, 50)
A = np.zeros((100,50))
for i in range(5):
    for j in range(5):
        A[i,j] = np.random.randint(0, 50)

# Execute function
gradient_descent(X, A, 10 ** -6, 1000)